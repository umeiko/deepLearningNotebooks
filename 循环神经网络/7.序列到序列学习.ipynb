{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import math\n",
    "import collections\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm\n",
    "import funcs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline import backend_inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36d03c20ebd42d9b9b74921e38fe344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11489286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_iter, src_vocab, tgt_vocab = funcs.load_data_nmt(batch_size=2, num_steps=8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 序列到序列学习\n",
    "\n",
    "机器翻译中的输入序列和输出序列都是长度可变的。 为了解决这类问题，使用通用的”编码器－解码器“架构。\n",
    "<p style=\"text-indent: 2em;\">遵循编码器－解码器架构的设计原则， 循环神经网络编码器使用长度可变的序列作为输入， 将其转换为固定形状的隐状态。 换言之，输入序列的信息被编码到循环神经网络编码器的隐状态中。 为了连续生成输出序列的词元， 独立的循环神经网络解码器是基于输入序列的编码信息 和输出序列已经看见的或者生成的词元来预测下一个词元。</p>\n",
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"./imgs/seq2seq.svg\" style=\"background-color: white;\">\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码器\n",
    "<p style=\"text-indent: 2em;\">\n",
    "    编码器将长度可变的输入序列转换成 形状固定的上下文变量 c ， 并且将输入序列的信息在该上下文变量中进行编码。\n",
    "    到目前为止，我们使用的是一个单向循环神经网络来设计编码器， 其中隐状态只依赖于输入子序列， 这个子序列是由输入序列的开始位置到隐状态所在的时间步的位置 （包括隐状态所在的时间步）组成。 我们也可以使用双向循环神经网络构造编码器， 其中隐状态依赖于两个输入子序列， 两个子序列是由隐状态所在的时间步的位置之前的序列和之后的序列 （包括隐状态所在的时间步）， 因此隐状态对整个序列的信息都进行了编码。\n",
    "</p>\n",
    "\n",
    "注意，我们使用了嵌入层（embedding layer） 来获得输入序列中每个词元的特征向量。 嵌入层的权重是一个矩阵， 其行数等于输入词表的大小（vocab_size）， 其列数等于特征向量的维度（embed_size）。 对于任意输入词元的索引$i$， 嵌入层获取权重矩阵的第$i$行（从0开始）以返回其特征向量。 另外，本文选择了一个多层门控循环单元来实现编码器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2SeqEncoder(funcs.Encoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout)\n",
    "    \n",
    "    def forward(self, X:torch.Tensor, *args):\n",
    "        # 数据输入为 (batch, timestep)，embedding之后将时间序列放在前面\n",
    "        X = self.embedding(X)\n",
    "        X = X.permute(1, 0, 2) \n",
    "        output, state = self.rnn(X)\n",
    "        # output的形状: (num_steps, batch_size, num_hiddens)\n",
    "        # state的形状 : (num_layers,batch_size, num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 4, 16]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "encoder.eval()\n",
    "X = torch.zeros((4, 7), dtype=torch.long)\n",
    "\n",
    "output, state = encoder(X)\n",
    "output.shape, state.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解码器\n",
    "编码器输出的上下文变量 $c$ 对整个输入序列进行编码，来自训练数据集的输出序列 $y_1, y_2, ..., y_T$ , 对于每个时间步 $t'$ 解码器输出 $y_{t'}$ 的概率取决于先前的输出子序列 $y_1, ..., y_{t'-1}$和上下文变量 $c$\n",
    "\n",
    "在获得解码器的隐状态之后，使用输出层和 softmax 操作来计算在时间步 $t'$ 时输出 $y_{t'}$ 的条件概率。\n",
    "\n",
    "当实现解码器时， 直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态。 这就要求使用循环神经网络实现的编码器和解码器具有相同数量的层和隐藏单元。 为了进一步包含经过编码的输入序列的信息， 上下文变量在所有的时间步与解码器的输入进行拼接（concatenate）。 为了预测输出词元的概率分布， 在循环神经网络解码器的最后一层使用全连接层来变换隐状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(funcs.Decoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size+num_hiddens, num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "    \n",
    "    def forward(self, X, state):\n",
    "        X:torch.Tensor = self.embedding(X)\n",
    "        # 数据输入为 (batch, timestep)，embedding之后将时间序列放在前面\n",
    "        X = X.permute(1, 0, 2)\n",
    "        # 复制context，使其具有与X相同的num_steps\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "        X_and_context = torch.cat((X, context), 2)\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        output = self.dense(output).permute(1,0,2)\n",
    "        # output的形状:(batch_size, num_steps, vocab_size)\n",
    "        # state的形状: (num_layers, batch_size,num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用与前面提到的编码器中相同的超参数来实例化解码器。 如我们所见，解码器的输出形状变为（批量大小，时间步数，词表大小）， 其中张量的最后一个维度存储预测的词元分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "decoder.eval()\n",
    "X = torch.zeros((4, 7), dtype=torch.long)\n",
    "state = decoder.init_state(encoder(X))\n",
    "output, state = decoder(X, state)\n",
    "output.shape, state.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"text-align: center;\">\n",
    "上述循环神经网络“编码器－解码器”模型中的各层\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"./imgs/seq2seq-details.svg\" style=\"background-color: white;\">\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "在每个时间步，解码器预测了输出词元的概率分布。 类似于语言模型，可以使用softmax来获得分布， 并通过计算交叉熵损失函数来进行优化。特定的填充词元被添加到序列的末尾， 因此不同长度的序列可以以相同形状的小批量加载。 但是，我们应该将填充词元的预测排除在损失函数的计算之外。\n",
    "\n",
    "为此，我们可以使用下面的`sequence_mask`函数 通过零值化屏蔽不相关的项， 以便后面任何不相关预测的计算都是与零的乘积，结果都等于零。\n",
    "\n",
    "在PyTorch中，符号`~`表示按位取反（bitwise NOT）的操作符。因此，`X[~mask]`将对张量`mask`中的每个元素执行按位取反操作，生成一个布尔张量（boolean tensor），然后将该布尔张量用作`X`张量的索引，从`X`张量中选择相应的元素。具体来说，如果`mask`张量的某个元素为0，则对应的`X`张量元素将被选择；如果`mask`张量的某个元素为非零值，则对应的`X`张量元素将被忽略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X:torch.Tensor, valid_len, value=0):\n",
    "    \"\"\"屏蔽序列中的无关项\"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 0],\n",
       "        [3, 4, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1,2,3,4], [3,4,5,6]])\n",
    "val_len = torch.tensor([3,2])\n",
    "sequence_mask(X, val_len, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过扩展softmax交叉熵损失函数来遮蔽不相关的预测。 最初，所有预测词元的掩码都设置为1。 一旦给定了有效长度，与填充词元对应的掩码将被设置为0。 最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"\n",
    "    输入参数\n",
    "    ---------------\n",
    "    `pred` : Tensor (batch_size, num_steps, vocab_size)\n",
    "    `label`: Tensor (batch_size, num_steps)\n",
    "    `valid_len`:Tensor (batch_size, )\n",
    "    \"\"\"\n",
    "    def forward(self, pred:torch.Tensor, label:torch.Tensor, valid_len:torch.Tensor):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        # self.reduction='none' 使得CE损失计算后不进行平均化处理\n",
    "        self.reduction='none'\n",
    "        unweighted_loss:torch.Tensor = super().forward(\n",
    "            pred.permute((0,2,1)), label\n",
    "        )\n",
    "        # 将无关项的损失变为0后再平均化\n",
    "        weightsed_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weightsed_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建三个相同的序列来进行代码健全性检查， 然后分别指定这些序列的有效长度为 4, 2, 0。 结果就是，第一个序列的损失应为第二个序列的两倍，而第三个序列的损失应为零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 1.1513, 0.0000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MaskedSoftmaxCELoss()\n",
    "loss(   torch.ones(3, 4, 10), \n",
    "        torch.ones((3, 4), dtype=torch.long),\n",
    "        torch.tensor([4, 2, 0])\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "在下面的循环训练过程中，特定的序列开始词元（`<bos>`）和 原始的输出序列（不包括序列结束词元`<eos>`） 拼接在一起作为解码器的输入。 这被称为强制教学（teacher forcing）， 因为原始的输出序列（词元的标签）被送入解码器。 或者，将来自上一个时间步的预测得到的词元作为解码器的当前输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(net:funcs.EncoderDecoder, data_iter, lr, num_epo, tgt_vocab, device):\n",
    "    \"\"\"\"\"\"\n",
    "    def xavier_init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if isinstance(m, nn.GRU):\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "    \n",
    "    net.apply(xavier_init_weights)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    net.train()\n",
    "    \n",
    "    l_list = []\n",
    "    clear_output(wait=True)\n",
    "    print(\"Training ... \")\n",
    "    for epoch in tqdm(range(num_epo)):\n",
    "        timer = funcs.Timer()\n",
    "        metric = funcs.Accumulator(2)  # 损失，词元数量\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = \\\n",
    "                [i.to(device) for i in batch]\n",
    "            # 强制教学 ，在每个标签句子的开头添加 \"<bos>\" 符号\n",
    "            bos =torch.tensor([tgt_vocab[\"<bos>\"]] * Y.shape[0], device=device).reshape(-1,1)\n",
    "            # 同时标签句子的尾端被去掉\n",
    "            dec_input = torch.cat((bos, Y[:, :-1]), 1)\n",
    "            Y_hat, _ = net(X, dec_input, X_valid_len)\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            l.sum().backward()\n",
    "            funcs.grad_clipping(net, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l.sum(), num_tokens)\n",
    "        l_list.append(float(metric[0] / metric[1]))\n",
    "        # if (epoch+1) % 10 == 0:\n",
    "            # clear_output(wait=True)\n",
    "            # print(f\"epo:{epoch + 1}: avg_loss:{(metric[0] / metric[1],)}\")\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
    "    f'tokens/sec on {str(device)}')\n",
    "    return l_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5179649954764ffbada5b6f83bca21ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.019, 35131.6 tokens/sec on cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f4b22ffd2a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"408.10125pt\" height=\"310.86825pt\" viewBox=\"0 0 408.10125 310.86825\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-03-05T09:42:35.088444</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 310.86825 \nL 408.10125 310.86825 \nL 408.10125 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 273.312 \nL 400.90125 273.312 \nL 400.90125 7.2 \nL 43.78125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m80c05d696c\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m80c05d696c\" x=\"60.013977\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(56.832727 287.910437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m80c05d696c\" x=\"114.304035\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 50 -->\n      <g transform=\"translate(107.941535 287.910437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m80c05d696c\" x=\"168.594093\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 100 -->\n      <g transform=\"translate(159.050343 287.910437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m80c05d696c\" x=\"222.884151\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 150 -->\n      <g transform=\"translate(213.340401 287.910437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m80c05d696c\" x=\"277.174208\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 200 -->\n      <g transform=\"translate(267.630458 287.910437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m80c05d696c\" x=\"331.464266\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 250 -->\n      <g transform=\"translate(321.920516 287.910437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#m80c05d696c\" x=\"385.754324\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 300 -->\n      <g transform=\"translate(376.210574 287.910437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- Epoch -->\n     <g transform=\"translate(207.030313 301.588562)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-45\" d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-45\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"63.183594\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"126.660156\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"187.841797\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"242.822266\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path id=\"m81eb337e7d\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m81eb337e7d\" x=\"43.78125\" y=\"270.972015\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.0 -->\n      <g transform=\"translate(20.878125 274.771233)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m81eb337e7d\" x=\"43.78125\" y=\"216.6572\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.1 -->\n      <g transform=\"translate(20.878125 220.456418)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m81eb337e7d\" x=\"43.78125\" y=\"162.342385\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 166.141603)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#m81eb337e7d\" x=\"43.78125\" y=\"108.027569\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.3 -->\n      <g transform=\"translate(20.878125 111.826788)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m81eb337e7d\" x=\"43.78125\" y=\"53.712754\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.4 -->\n      <g transform=\"translate(20.878125 57.511973)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- Loss -->\n     <g transform=\"translate(14.798438 151.223187)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-4c\" d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"53.962891\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"115.144531\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"167.244141\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path d=\"M 60.013977 19.296 \nL 61.099778 85.199739 \nL 62.18558 109.65383 \nL 63.271381 122.771076 \nL 64.357182 132.472347 \nL 65.442983 138.535904 \nL 69.786188 158.570462 \nL 71.95779 167.810806 \nL 76.300995 182.219845 \nL 77.386796 184.867485 \nL 78.472597 188.434792 \nL 81.73 196.673302 \nL 82.815802 199.062142 \nL 83.901603 201.831435 \nL 84.987404 204.0926 \nL 86.073205 205.957914 \nL 87.159006 208.466101 \nL 90.41641 214.687446 \nL 91.502211 217.111169 \nL 92.588012 218.007881 \nL 94.759614 221.846699 \nL 96.931217 224.262215 \nL 98.017018 226.137565 \nL 101.274421 229.211183 \nL 103.446023 231.423021 \nL 104.531825 232.224326 \nL 106.703427 234.536593 \nL 107.789228 235.078905 \nL 109.96083 236.626628 \nL 111.046632 238.220307 \nL 112.132433 238.704537 \nL 113.218234 239.499624 \nL 114.304035 239.217967 \nL 116.475637 240.477024 \nL 117.561439 241.593274 \nL 118.64724 242.328833 \nL 120.818842 243.466115 \nL 122.990444 244.511027 \nL 124.076245 245.651241 \nL 125.162047 245.631466 \nL 126.247848 246.0487 \nL 127.333649 246.800705 \nL 129.505251 247.593735 \nL 130.591052 248.352724 \nL 131.676854 248.321356 \nL 132.762655 248.901005 \nL 133.848456 249.303507 \nL 134.934257 249.089305 \nL 136.020058 249.529966 \nL 137.105859 249.585732 \nL 139.277462 250.597089 \nL 140.363263 250.982754 \nL 141.449064 250.882948 \nL 142.534865 251.548417 \nL 143.620666 251.657104 \nL 145.792269 252.338569 \nL 146.87807 252.321545 \nL 149.049672 252.673844 \nL 150.135473 253.382405 \nL 152.307075 253.39948 \nL 153.392877 254.028788 \nL 155.564479 254.162416 \nL 156.65028 254.075375 \nL 157.736081 254.754313 \nL 158.821882 254.610251 \nL 162.079286 255.175057 \nL 163.165087 254.786483 \nL 164.250888 255.355876 \nL 165.336689 255.377135 \nL 166.42249 255.61753 \nL 167.508292 256.030807 \nL 168.594093 256.154026 \nL 170.765695 256.057459 \nL 171.851496 255.722549 \nL 172.937297 256.563719 \nL 174.023099 256.190026 \nL 175.1089 256.807929 \nL 176.194701 256.505524 \nL 177.280502 256.657356 \nL 178.366303 256.476176 \nL 179.452104 256.857075 \nL 180.537906 256.938927 \nL 181.623707 257.35607 \nL 182.709508 257.14332 \nL 184.88111 257.433433 \nL 185.966911 257.24643 \nL 187.052712 257.868321 \nL 189.224315 257.471588 \nL 190.310116 257.681681 \nL 192.481718 257.606587 \nL 193.567519 258.101515 \nL 194.653321 257.987635 \nL 195.739122 257.637747 \nL 196.824923 258.233167 \nL 197.910724 257.903828 \nL 198.996525 258.213386 \nL 200.082326 258.236849 \nL 201.168127 258.423618 \nL 202.253929 258.310068 \nL 203.33973 258.365449 \nL 204.425531 258.71713 \nL 205.511332 258.367508 \nL 206.597133 258.472008 \nL 207.682934 258.704459 \nL 208.768736 258.823114 \nL 209.854537 258.664887 \nL 210.940338 258.899481 \nL 212.026139 258.753709 \nL 214.197741 259.025349 \nL 215.283542 258.662292 \nL 216.369344 259.379073 \nL 217.455145 258.765322 \nL 218.540946 258.906857 \nL 219.626747 259.320782 \nL 220.712548 258.983559 \nL 221.798349 258.8416 \nL 222.884151 259.073238 \nL 223.969952 259.001846 \nL 225.055753 259.105972 \nL 226.141554 259.518687 \nL 227.227355 259.22909 \nL 228.313156 259.079983 \nL 229.398958 259.244887 \nL 230.484759 259.131506 \nL 231.57056 259.380331 \nL 233.742162 259.28349 \nL 234.827963 259.419006 \nL 235.913764 259.349768 \nL 238.085367 259.646057 \nL 239.171168 259.369416 \nL 240.256969 259.307647 \nL 241.34277 259.683755 \nL 243.514373 259.752476 \nL 244.600174 259.512146 \nL 245.685975 259.685937 \nL 246.771776 259.517624 \nL 247.857577 259.878544 \nL 248.943378 259.743988 \nL 250.029179 260.073484 \nL 251.114981 259.939421 \nL 252.200782 259.974853 \nL 255.458185 259.776807 \nL 256.543986 259.958283 \nL 258.715589 259.634235 \nL 259.80139 259.96788 \nL 260.887191 259.819796 \nL 261.972992 260.006451 \nL 264.144594 259.921815 \nL 265.230396 260.181488 \nL 266.316197 260.244044 \nL 267.401998 260.187812 \nL 268.487799 260.265987 \nL 270.659401 259.94698 \nL 272.831004 260.048526 \nL 273.916805 259.980815 \nL 275.002606 260.212513 \nL 277.174208 260.040011 \nL 278.26001 260.258615 \nL 279.345811 260.174827 \nL 281.517413 260.32274 \nL 282.603214 260.118371 \nL 283.689015 260.113884 \nL 284.774816 260.46748 \nL 285.860618 260.14858 \nL 286.946419 260.470127 \nL 288.03222 260.276164 \nL 290.203822 260.656905 \nL 291.289623 260.347348 \nL 292.375425 260.485128 \nL 293.461226 260.348763 \nL 295.632828 260.268733 \nL 296.718629 260.520808 \nL 297.80443 260.35875 \nL 298.890231 260.481706 \nL 299.976033 260.466626 \nL 301.061834 260.578081 \nL 302.147635 260.494327 \nL 303.233436 260.265504 \nL 307.576641 260.653136 \nL 308.662442 260.46804 \nL 309.748243 260.422035 \nL 310.834044 260.643346 \nL 311.919845 260.464024 \nL 315.177249 260.687146 \nL 317.348851 260.405357 \nL 318.434652 260.66751 \nL 319.520453 260.466105 \nL 320.606255 260.735579 \nL 321.692056 260.648637 \nL 322.777857 260.771695 \nL 324.949459 260.699864 \nL 326.03526 260.807781 \nL 328.206863 260.750208 \nL 329.292664 260.368479 \nL 330.378465 260.770823 \nL 331.464266 260.771167 \nL 332.550067 260.563814 \nL 336.893272 260.585606 \nL 339.064874 260.799944 \nL 340.150675 260.667701 \nL 341.236477 260.833432 \nL 342.322278 260.676315 \nL 344.49388 260.844337 \nL 347.751283 260.523354 \nL 348.837085 260.729767 \nL 349.922886 260.608495 \nL 351.008687 260.771886 \nL 352.094488 260.540568 \nL 355.351892 260.794484 \nL 356.437693 260.47547 \nL 357.523494 260.565661 \nL 358.609295 260.897028 \nL 360.780897 260.769352 \nL 361.866698 260.879754 \nL 362.9525 260.710419 \nL 364.038301 260.955647 \nL 366.209903 260.551418 \nL 367.295704 260.616065 \nL 369.467307 261.04764 \nL 370.553108 260.881301 \nL 371.638909 260.444172 \nL 372.72471 260.510649 \nL 373.810511 260.799337 \nL 374.896312 260.784012 \nL 377.067915 260.916905 \nL 378.153716 260.514642 \nL 379.239517 260.487114 \nL 380.325318 260.907595 \nL 381.411119 260.671155 \nL 382.49692 261.216 \nL 384.668523 260.642515 \nL 384.668523 260.642515 \n\" clip-path=\"url(#pe26a24f743)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 273.312 \nL 43.78125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 400.90125 273.312 \nL 400.90125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 273.312 \nL 400.90125 273.312 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 7.2 \nL 400.90125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 338.62625 29.878125 \nL 393.90125 29.878125 \nQ 395.90125 29.878125 395.90125 27.878125 \nL 395.90125 14.2 \nQ 395.90125 12.2 393.90125 12.2 \nL 338.62625 12.2 \nQ 336.62625 12.2 336.62625 14.2 \nL 336.62625 27.878125 \nQ 336.62625 29.878125 338.62625 29.878125 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_14\">\n     <path d=\"M 340.62625 20.298437 \nL 350.62625 20.298437 \nL 360.62625 20.298437 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_15\">\n     <!-- train -->\n     <g transform=\"translate(368.62625 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pe26a24f743\">\n   <rect x=\"43.78125\" y=\"7.2\" width=\"357.12\" height=\"266.112\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs, device = 0.005, 300, \"cuda:0\"\n",
    "\n",
    "print(\"Loading dataset... \")\n",
    "train_iter, src_vocab, tgt_vocab = funcs.load_data_nmt(batch_size, num_steps)\n",
    "encoder = seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,\n",
    "                        dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,\n",
    "                        dropout)\n",
    "\n",
    "net = funcs.EncoderDecoder(encoder, decoder)\n",
    "l_list = train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)\n",
    "\n",
    "plt.title(\"Seq2Seq Loss Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\") \n",
    "plt.plot(l_list)\n",
    "plt.legend([\"train\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测\n",
    "为了采用一个接着一个词元的方式预测输出序列， 每个解码器当前时间步的输入都将来自于前一时间步的预测词元。 与训练类似，序列开始词元（`<bos>`） 在初始时间步被输入到解码器中。 该预测过程如图所示， 当输出序列的预测遇到序列结束词元（`<eos>`）时，预测就结束了。\n",
    "<p style=\"text-align: center;\">\n",
    "使用循环神经网络编码器-解码器逐词元地预测输出序列\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"./imgs/seq2seq-predict.svg\" style=\"background-color: white;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seq2seq(net:funcs.EncoderDecoder, src_sentence:str, src_vocab:funcs.Vocab, \n",
    "                    tgt_vocab:funcs.Vocab, max_steps:int, device:str, save_attention_weights=False):\n",
    "    \n",
    "    net.eval()\n",
    "    src_sentence = \"<bos> \" + src_sentence if not src_sentence.startswith(\"<bos> \") else src_sentence\n",
    "    src_sentence_tensor, _ = funcs.build_array_nmt([src_sentence.lower().split(\" \")], src_vocab, max_steps)\n",
    "    src_sentence_tensor = src_sentence_tensor.to(device)\n",
    "    enc_output = net.encoder(src_sentence_tensor,)\n",
    "    state      = net.decoder.init_state(enc_output)\n",
    "\n",
    "    outputs   = [src_vocab[\"<bos>\"]]\n",
    "    # 每次以输出序列的最后一个字符作为输入\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1,1))\n",
    "    attention_weight_seq = []\n",
    "    # 这个阶段为预测阶段\n",
    "    for _ in range(max_steps):\n",
    "        y_hat, state = net.decoder.forward(get_input(), state)\n",
    "        outputs.append(int(y_hat.argmax(dim=-1).reshape(1)))\n",
    "        \n",
    "        # 保存注意力权重（稍后讨论）\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights)\n",
    "        \n",
    "        if outputs[-1] == tgt_vocab[\"<eos>\"]:\n",
    "            outputs.pop()\n",
    "            outputs.pop(0)\n",
    "            break\n",
    "    return ''.join([tgt_vocab.idx_to_token[i] + \" \" for i in outputs])[:-1], attention_weight_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"j'ai perdu .\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_seq2seq(net, \"i lost .\", src_vocab, tgt_vocab, 15, device)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测序列的评估\n",
    "可以通过与真实的标签序列进行比较来评估预测序列。 虽然 (Papineni et al., 2002) 提出的`BLEU`（bilingual evaluation understudy） 最先是用于评估机器翻译的结果， 但现在它已经被广泛用于测量许多应用的输出序列的质量。 原则上说，对于预测序列中的任意 $n$ 元语法（n-grams）， BLEU的评估都是这个 $n$ 元语法是否出现在标签序列中。\n",
    "BLUE定义为： \n",
    "$$\n",
    "exp(min(0,1 - \\frac{len_{label}}{len_{pred}})) \\prod \\limits_{n=1}^k p_{n}^{1/2^n}\n",
    "$$ \n",
    "其中$len_{label}$表示标签序列中的词元数$len_{pred}$表示预测序列中的词元数,k是用于匹配的最长的n元语法。用$p_n$表示n元语法的精确度，它是两个数量的比值：\n",
    "$$\n",
    "p_n = \\frac{预测序列与标签序列中匹配的n元语法的数量}{预测序列中n元语法的数量的比率。} \n",
    "$$\n",
    "当预测序列与标签序列完全相同时，BLEU为1。此外，由于n元语法越长则匹配难度越大， 所以BLEU为更长的n元语法的精确度分配更大的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(pred_seq:str, label_seq:str, k:float):\n",
    "    '''\n",
    "    计算BLEU\n",
    "    `pred_seq`  : 预测的文本序列\n",
    "    `label_seq` : 标签文本序列\n",
    "    `k`         : 最大统计语法元数\n",
    "    '''\n",
    "    pred_tokens, label_tokens = pred_seq.split(\" \"),  label_seq.split(\" \")\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    score = math.exp(min(0, 1 - len_label/len_pred))\n",
    "\n",
    "    for n in range(1, k + 1):\n",
    "        # 计算 n 元语法中的 P_n 值\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "        for i in range(len_pred - n + 1):\n",
    "            # 在标签中取出所有的 n 元语法，并计数。\n",
    "            label_subs[\" \".join(label_tokens[i : i + n])] += 1\n",
    "        for i in range(len_pred - n + 1):\n",
    "            # 在预测值中寻找有没有匹配上的，包含匹配多次的情况\n",
    "            if label_subs[\" \".join(pred_tokens[i : i + n])] > 0:\n",
    "                num_matches += 1\n",
    "                # 匹配计数器，已经匹配过的计数减少\n",
    "                label_subs[\" \".join(pred_seq[i : i + n])] -= 1\n",
    "        score *= math.pow(num_matches / (len_pred - n + 1 + 1e-5), math.pow(0.5, n))\n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`defaultdic` 是 `dict` 的一个子类，它具有与 `dict` 类相同的所有方法，但是在以下方面有所不同：\n",
    "\n",
    "1. 默认值： `defaultdict` 允许你指定一个默认值，这样当你访问字典中不存在的键时，它会返回这个默认值，而不是抛出 `KeyError` 异常。这个默认值可以是任何可调用对象，如 `int`、`list`、`lambda` 等。\n",
    "\n",
    "2. 自动初始化：当你使用一个不存在的键来访问一个 `defaultdict` 时，它会自动初始化该键，并为它赋值为默认值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . \t=> assieds-toi !\t bleu: 0.000\n",
      "i lost . \t=> j'ai perdu .\t bleu: 1.000\n",
      "he's calm . \t=> va doucement !\t bleu: 0.000\n",
      "i'm home . \t=> je suis du <unk> !\t bleu: 0.447\n"
     ]
    }
   ],
   "source": [
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, attention_weight_seq = predict_seq2seq(\n",
    "        net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
    "    print(f'{eng} \\t=> {translation}\\t bleu: {bleu(translation, fra, k=2):.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "- 根据“编码器-解码器”架构的设计， 我们可以使用两个循环神经网络来设计一个序列到序列学习的模型。\n",
    "- 我们可以使用遮蔽来过滤不相关的计算，例如在计算损失时。\n",
    "- 在“编码器－解码器”训练中，强制教学方法将原始输出序列（而非预测结果）输入解码器。\n",
    "- BLEU是一种常用的评估方法，它通过测量预测序列和标签序列之间的n元语法的匹配度来评估预测。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d2cc74a804977258fca08641e7f8c71be122cbd2b1d46246183378bd8e471b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
