{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline import backend_inline\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Callable\n",
    "import funcs\n",
    "\n",
    "backend_inline.set_matplotlib_formats('svg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隐状态中 $ X_{t}  W_{xh} + H_{t-1}  W_{hh} $的计算，相当于$ X_{t} $与$ H_{t-1} $在列的拼接和$ W_{xh}$与$W_{hh} $在行拼接的乘法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9002,  1.7100, -3.0632, -0.0411],\n",
      "        [ 3.8465, -3.9676, -1.7453, -1.4797],\n",
      "        [-0.5631, -1.5626,  1.3826,  0.4590]])\n",
      "tensor([[ 0.9002,  1.7100, -3.0632, -0.0411],\n",
      "        [ 3.8465, -3.9676, -1.7453, -1.4797],\n",
      "        [-0.5631, -1.5626,  1.3826,  0.4590]])\n"
     ]
    }
   ],
   "source": [
    "X, W_xh = torch.normal(0,1,(3,1)), torch.normal(0,1,(1,4))\n",
    "H, W_hh = torch.normal(0,1,(3,4)), torch.normal(0,1,(4,4))\n",
    "print(X @ W_xh + H @ W_hh)\n",
    "print(torch.cat((X, H), dim=1) @ torch.cat((W_xh, W_hh), dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据形状X为 (小批量, 时间序列) , 将其做词向量编码时进行转置处理, 将时间序列作为第一轴, 方便按时间访问数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5]) torch.Size([5, 3, 28])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(0,15).reshape((3, 5))\n",
    "X_hot = F.one_hot(X.T, 28)\n",
    "print(X.shape, X_hot.shape)\n",
    "del X, X_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始进行循环神经网络的实现, 定义超参数: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<TimeMachineLoader with 10000 corpus: \"t\", \"h\", \"e\", \" \", \"t\", ...>,\n",
       " <Vocab with 28 tokens: \"<unk>\", \" \", \"e\", \"t\", \"a\", ...>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = funcs.load_data_time_machine(\n",
    "    batch_size, num_steps, token_type=\"char\")\n",
    "train_iter, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化循环神经网络的模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_ = Callable[[int, int, str], list]\n",
    "def get_rnn_params(vocab_size:int, num_hiddens:int, device:str=\"cuda:0\"):\n",
    "    num_inputs = num_outputs = vocab_size  # 针对one-hot编码，所以维度与字典长度相等\n",
    "    normal = lambda shape: torch.randn(size=shape, device=device) * 0.01\n",
    "    # 隐藏层的参数\n",
    "    # [time_steps, b, embedding] -> [time_steps, b, h]\n",
    "    W_xh = normal((num_inputs, num_hiddens))\n",
    "    # [b, h] -> [h, h]\n",
    "    W_hh = normal((num_hiddens, num_hiddens))\n",
    "    b_h = torch.zeros(num_hiddens, device=device)\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q  = torch.zeros(num_outputs, device=device)\n",
    "    # 赋予梯度\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params\n",
    "\n",
    "state_ = Callable[[int, int, str], tuple]\n",
    "def init_rnn_state(batch_size:int, num_hiddens:int, device:str):\n",
    "    \"\"\"初始化隐藏状态\"\"\"\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$RNN$的输出由隐状态得出:  \n",
    "- $H_{t} = X_{t}  W_{xh} + H_{t-1}  W_{hh} \\\\ O_{t} = H_{t} W_{hq} + b_{q}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_ = Callable[[torch.Tensor, torch.Tensor, list], tuple]\n",
    "def rnn_forward(inputs:torch.Tensor, state:torch.Tensor, params:list):\n",
    "    \"\"\"在一个时间步内计算隐藏状态和输出\"\"\"\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    out_puts = []\n",
    "    # inputs : [time_steps, b, embedding]\n",
    "    for X in inputs:\n",
    "        state = torch.tanh(X @ W_xh + state @ W_hh + b_h)\n",
    "        Y = state @ W_hq + b_q\n",
    "        out_puts.append(Y)\n",
    "    return torch.cat(out_puts, dim=0), (state, )\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \"\"\"循环神经网络的简单实现\"\"\"\n",
    "    def __init__(self, vocab_size:int, num_hiddens:int, device:str, \n",
    "                    get_params:param_, init_state:state_, \n",
    "                    forward_fn:forward_) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens, device)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "    \n",
    "    def forward(self, x:torch.Tensor, state:torch.Tensor):\n",
    "        x = F.one_hot(x.T, self.vocab_size).to(torch.float32)\n",
    "        return self.forward_fn(x, state, self.params)\n",
    "        \n",
    "    def begin_state(self, batch_size:int, device:str):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        out = f\"<RNN_Module with {self.num_hiddens} hiddens>\"\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查模型输出形状是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1120, 28]), torch.Size([32, 512]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens = 512\n",
    "device = \"cuda:0\"\n",
    "rnn = RNN(len(vocab), num_hiddens, device, get_rnn_params, init_rnn_state, rnn_forward)\n",
    "x, y = next(iter(train_iter))\n",
    "state = rnn.begin_state(train_iter.batch_size, device)\n",
    "Y, new_state = rnn(torch.tensor(x).to(device), state)\n",
    "Y.shape, new_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d2cc74a804977258fca08641e7f8c71be122cbd2b1d46246183378bd8e471b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
