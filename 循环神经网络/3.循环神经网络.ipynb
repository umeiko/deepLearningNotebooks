{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络\n",
    "从零实现循环神经网络，并了解其内部的细节问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib_inline import backend_inline\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Callable, Iterator\n",
    "import funcs\n",
    "\n",
    "backend_inline.set_matplotlib_formats('svg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隐状态中 $ X_{t}  W_{xh} + H_{t-1}  W_{hh} $的计算，相当于$ X_{t} $与$ H_{t-1} $在列的拼接和$ W_{xh}$与$W_{hh} $在行拼接的乘法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0389, -1.3947,  0.6169,  0.9905],\n",
      "        [-0.1577,  1.1174,  0.1550, -2.8558],\n",
      "        [-1.7200,  0.5513, -0.6273, -1.1522]])\n",
      "tensor([[ 1.0389, -1.3947,  0.6169,  0.9905],\n",
      "        [-0.1577,  1.1174,  0.1550, -2.8558],\n",
      "        [-1.7200,  0.5513, -0.6273, -1.1522]])\n"
     ]
    }
   ],
   "source": [
    "X, W_xh = torch.normal(0,1,(3,1)), torch.normal(0,1,(1,4))\n",
    "H, W_hh = torch.normal(0,1,(3,4)), torch.normal(0,1,(4,4))\n",
    "print(X @ W_xh + H @ W_hh)\n",
    "print(torch.cat((X, H), dim=1) @ torch.cat((W_xh, W_hh), dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据形状 $X$ 为 (小批量, 时间序列) , 将其做词向量编码时进行转置处理, 将时间序列作为第一轴, 方便按时间访问数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5]) torch.Size([5, 3, 28])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(0,15).reshape((3, 5))\n",
    "X_hot = F.one_hot(X.T, 28)\n",
    "print(X.shape, X_hot.shape)\n",
    "del X, X_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始进行循环神经网络的实现, 定义超参数: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<TimeMachineLoader with 10000 corpus: \"t\", \"h\", \"e\", \" \", \"t\", ...>,\n",
       " <Vocab with 28 tokens: \"<unk>\", \" \", \"e\", \"t\", \"a\", ...>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = funcs.load_data_time_machine(\n",
    "    batch_size, num_steps, token_type=\"char\")\n",
    "train_iter, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化循环神经网络的模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ = Callable[[torch.Tensor,torch.Tensor], torch.Tensor]\n",
    "param_ = Callable[[int, int, str], list]\n",
    "def get_rnn_params(vocab_size:int, num_hiddens:int, device:str=\"cuda:0\"):\n",
    "    num_inputs = num_outputs = vocab_size  # 针对one-hot编码，所以维度与字典长度相等\n",
    "    normal = lambda shape: torch.randn(size=shape, device=device) * 0.01\n",
    "    # 隐藏层的参数\n",
    "    # [time_steps, b, embedding] -> [time_steps, b, h]\n",
    "    W_xh = normal((num_inputs, num_hiddens))\n",
    "    # [b, h] -> [h, h]\n",
    "    W_hh = normal((num_hiddens, num_hiddens))\n",
    "    b_h = torch.zeros(num_hiddens, device=device)\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q  = torch.zeros(num_outputs, device=device)\n",
    "    # 赋予梯度\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params\n",
    "\n",
    "state_ = Callable[[int, int, str], torch.Tensor]\n",
    "def init_rnn_state(batch_size:int, num_hiddens:int, device:str):\n",
    "    \"\"\"初始化隐藏状态\"\"\"\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$RNN$的输出由隐状态得出:  \n",
    "- $H_{t} = X_{t}  W_{xh} + H_{t-1}  W_{hh} \\\\ O_{t} = H_{t} W_{hq} + b_{q}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_ = Callable[[torch.Tensor, torch.Tensor, list], tuple]\n",
    "def rnn_forward(inputs:torch.Tensor, state:torch.Tensor, params:list):\n",
    "    \"\"\"在一个时间步内计算隐藏状态和输出\"\"\"\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    out_puts = []\n",
    "    # inputs : [time_steps, b, embedding]\n",
    "    for X in inputs:\n",
    "        state = torch.tanh(X @ W_xh + state @ W_hh + b_h)\n",
    "        Y = state @ W_hq + b_q\n",
    "        out_puts.append(Y)\n",
    "    return torch.cat(out_puts, dim=0), state\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \"\"\"循环神经网络的简单实现\"\"\"\n",
    "    def __init__(self, vocab_size:int, num_hiddens:int, device:str, \n",
    "                    get_params:param_, init_state:state_, \n",
    "                    forward_fn:forward_) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens, device)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "    \n",
    "    def forward(self, x:torch.Tensor, state:torch.Tensor):\n",
    "        x = F.one_hot(x.T, self.vocab_size).to(torch.float32)\n",
    "        return self.forward_fn(x, state, self.params)\n",
    "        \n",
    "    def begin_state(self, batch_size:int, device:str):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)\n",
    "    \n",
    "    def parameters(self, recurse: bool = True):\n",
    "        for param in self.params:\n",
    "            yield param\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        out = f\"<RNN_Module with {self.num_hiddens} hiddens>\"\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查模型输出形状是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1120, 28]), torch.Size([512]))"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens = 512\n",
    "device = \"cuda:0\"\n",
    "rnn = RNN(len(vocab), num_hiddens, device, get_rnn_params, init_rnn_state, rnn_forward)\n",
    "x, y = next(iter(train_iter))\n",
    "state = rnn.begin_state(train_iter.batch_size, device)\n",
    "Y, new_state = rnn(torch.tensor(x).to(device), state)\n",
    "Y.shape, new_state[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测\n",
    "- 定义一个预测函数，用来生成prefix后的新字符。其中的prefix是一个用户提供的包含多个字符的字符串。 在循环遍历prefix中的开始字符时， 不断地将隐状态传递到下一个时间步，但是不生成任何输出。 这被称为预热（warm-up）期， 因为在此期间模型会自我更新（例如，更新隐状态）， 但不会进行预测。 预热期结束后，隐状态的值通常比刚开始的初始值更适合预测， 从而预测字符并输出它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prefix, num_preds:int, net:RNN, vocab:funcs.Vocab, device:str):\n",
    "    state     = net.begin_state(batch_size=1, device=device)\n",
    "    outputs   = [vocab[prefix[0]]]\n",
    "    # 每次以输出序列的最后一个字符作为输入\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1,1))\n",
    "    # 这个阶段作为预热阶段\n",
    "    for y in prefix[1:]:\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    # 这个阶段为预测阶段\n",
    "    for _ in range(num_preds):\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time machine and nepcwidskyuksnepcwid'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"time machine and \", 20, rnn, vocab, \"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度裁剪\n",
    "为了防止梯度过大，在链式传播的过程中产生爆炸，对梯度的进行裁剪：\n",
    "\n",
    "$ g \\gets min(1, \\frac{\\theta}{||g||}) \\cdot g $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net:nn.Module, theta:float):\n",
    "    \"\"\"梯度裁剪\"\"\"\n",
    "    params = [p for p in net.parameters() if p.requires_grad]\n",
    "    # 计算 ||g||\n",
    "    norm = torch.sqrt( sum(torch.sum(p.grad ** 2) for p in params) )\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码演示了梯度裁剪的实际效果。线性函数中的偏置 $w$ 的梯度被裁剪到了 $\\theta$ 范围内"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8032]])\n",
      "tensor([[1.0000]])\n"
     ]
    }
   ],
   "source": [
    "l1 = nn.Linear(1,1,False)\n",
    "loss = (l1(torch.tensor([1.])) ** 2).sum()\n",
    "loss.backward()\n",
    "print(l1.weight.grad)\n",
    "grad_clipping(l1, 1.)\n",
    "print(l1.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "循环神经网络的训练与多层感知机存在部分差异:\n",
    "1. 序列数据的不同采样方法会导致隐状态初始化的差异\n",
    "2. 需要在更新模型前裁剪梯度，保证某点处发生的梯度爆炸不会导致模型发散\n",
    "3. 用困惑度评价模型，确保不同长度的序列仍然具有可比性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    \"\"\"简易计时器\"\"\"\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def stop(self)-> float:\n",
    "        return time.time() - self.start_time\n",
    "\n",
    "def train(net:RNN, train_iter:funcs.SeqDataLoaderTimeMachine, loss:loss_, opt:torch.optim.Optimizer, device:str, use_random_iter:bool):\n",
    "    \"\"\"返回结果:困惑度, 训练速度 (词元/秒) \"\"\"\n",
    "    state = None\n",
    "    loss_count, num_tokens = 0, 0\n",
    "    timer = Timer()\n",
    "    for x, y in train_iter:\n",
    "        # 需要初始化隐变量的情况\n",
    "        if state is None or use_random_iter:\n",
    "            state = net.begin_state(x.shape[0], device)\n",
    "        else:\n",
    "            state = state.detach()\n",
    "        # 转置后展平，标签数据按照时序排列\n",
    "        y = torch.tensor(y).T.reshape(-1)\n",
    "        x, y = torch.tensor(x).to(device), y.to(device)\n",
    "        y_hat, state = net(x, state)\n",
    "        l = loss(y_hat, y.to(torch.float32)).sum()\n",
    "        # 更新梯度\n",
    "        opt.zero_grad()\n",
    "        l.backward()\n",
    "        grad_clipping(net, 1.)\n",
    "        opt.step()\n",
    "        # .numel() 返回张量中的所有参数数量\n",
    "        loss_count += l * y.numel()\n",
    "        num_tokens += y.numel()\n",
    "    return np.exp(loss_count / num_tokens), num_tokens / timer.stop()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d2cc74a804977258fca08641e7f8c71be122cbd2b1d46246183378bd8e471b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
