{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26902988",
   "metadata": {},
   "source": [
    "# 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4966ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1802fe42",
   "metadata": {},
   "source": [
    "## 将数据集提取并简单处理\n",
    "从H.G.Well的时光机器中加载文本。 这是一个相当小的语料库，只有30000多个单词， 而现实中的文档集合可能会包含数十亿个单词。 下面的函数将数据集读取到由多条文本行组成的列表中，其中每条文本行都是一个字符串。 在这里忽略了标点符号和字母大写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "812100b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data_set/H_G_Well_time_machine.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "# 忽略标点符号和字母大写\n",
    "out = []\n",
    "for line in lines:\n",
    "    # ^取非 +连续选取多个匹配\n",
    "    line = re.sub('[^A-Za-z]+', ' ', line).strip().lower()\n",
    "    if line:\n",
    "        out.append(line)\n",
    "lines = out\n",
    "del(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d888cc",
   "metadata": {},
   "source": [
    "## 词元化\n",
    "将文本序列拆分为词元(token), 将一个句子转化为词元序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ddf9a9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "['this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and']\n",
      "['most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions']\n",
      "['whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms']\n",
      "['of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines, token='word'):\n",
    "    \"\"\"将文本行拆分为单词或字符词元\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('错误：未知词元类型：' + token)\n",
    "\n",
    "tokens = tokenize(lines)\n",
    "for i in range(5):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073079fb",
   "metadata": {},
   "source": [
    "## 词表\n",
    "词元的类型是字符串，而模型需要的输入是数字。构建一个字典，通常也叫做词表（vocabulary）， 用来将字符串类型的词元映射到从开始的数字索引中。\n",
    "-  先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计， 得到的统计结果称之为语料（corpus）。 然后根据每个唯一词元的出现频率，为其分配一个数字索引。 很少出现的词元通常被移除，这可以降低复杂性。 另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“\\<unk\\>”。 我们可以选择增加一个列表，用于保存那些被保留的词元， 例如：填充词元（“\\<pad\\>”）； 序列开始词元（“\\<bos\\>”）； 序列结束词元（“\\<eos\\>”）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "763b61ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"一个词元字典类的实现\"\"\"\n",
    "    def __init__(self, tokens:list, min_freq=0, reserved_tokens:list=None) -> None:\n",
    "        if tokens is not None:\n",
    "            # 当第一个条件满足时，就不会跳到第二个判断，避免了空列表报错的情况。\n",
    "            if len(tokens)!=0 and isinstance(tokens[0], list):\n",
    "                tokens = [i for line in tokens for i in line]\n",
    "        else:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        counter=collections.Counter(tokens)\n",
    "        # 按出现词频从高到低排序\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x:x[1], reverse=True)\n",
    "        # 通过列表,利用序号访问词元。\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens # 未知词元<unk>的索引为0, 保留词元排在最前\n",
    "        self.token_to_idx = {\n",
    "            i: k\n",
    "            for k, i in enumerate(self.idx_to_token) \n",
    "        }\n",
    "        \n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:  # 过滤掉出现频率低于要求的词\n",
    "                break\n",
    "            if token not in self.token_to_idx:  \n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, input_tokens):\n",
    "        \"\"\"输入单字串或序列, 将其全部转化为序号编码\"\"\"\n",
    "        if isinstance(input_tokens, str):\n",
    "            return self.token_to_idx.get(input_tokens, 0)\n",
    "        return [self.__getitem__(token) for token in input_tokens]\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        show_items = 5 if len(self) > 5 else len(self)\n",
    "        out = f\"<Vocab with {len(self)} tokens: \"\n",
    "        for i in range(show_items):\n",
    "            out += f'\"{self.idx_to_token[i]}\", '\n",
    "        out += \"...>\"\n",
    "        return out\n",
    "\n",
    "    def to_tokens(self, input_keys):\n",
    "        \"\"\"输入单s索引或序列, 将其全部转化为词元\"\"\"\n",
    "        if isinstance(input_keys, int):\n",
    "            return self.idx_to_token[input_keys] if input_keys < len(self) else self.idx_to_token[0]\n",
    "        elif isinstance(input_keys, (list, tuple)):\n",
    "            return [self.to_tokens(keys) for keys in input_keys]\n",
    "        else:\n",
    "            return self.idx_to_token[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bb95fa",
   "metadata": {},
   "source": [
    "# 整合\n",
    "函数返回corpus（词元索引列表）和vocab（时光机器语料库的词表）。使用字符（而不是单词）实现文本词元化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1323cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(lines, \"char\")\n",
    "vocab = Vocab(tokens)\n",
    "corpus = [vocab[token] for line in tokens for token in line]\n",
    "len(corpus), vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
